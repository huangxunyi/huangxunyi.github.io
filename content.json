{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"Asm","slug":"asm","date":"2019-10-04T06:29:32.000Z","updated":"2019-10-04T14:29:32.000Z","comments":true,"path":"2019/10/04/asm/","link":"","permalink":"http://yoursite.com/2019/10/04/asm/","excerpt":"Cut out summary from your post content here.","text":"Cut out summary from your post content here. The remaining content of your post.","categories":[],"tags":[{"name":"tagA","slug":"tagA","permalink":"http://yoursite.com/tags/tagA/"},{"name":"tagB","slug":"tagB","permalink":"http://yoursite.com/tags/tagB/"}],"author":"Author Name"},{"title":"Jvm","slug":"jvm","date":"2019-10-02T20:17:11.000Z","updated":"2019-10-04T14:27:30.000Z","comments":true,"path":"2019/10/03/jvm/","link":"","permalink":"http://yoursite.com/2019/10/03/jvm/","excerpt":"","text":"JVM对象存活- 引用计数- 可达性分析 GC Root - 回收方法区垃圾回收- 标记-清除 (Make-Sweep)- 复制- 标记-整理- 分代回收JVM类加载机制","categories":[],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"},{"name":"GC","slug":"GC","permalink":"http://yoursite.com/tags/GC/"},{"name":"JMM","slug":"JMM","permalink":"http://yoursite.com/tags/JMM/"}],"author":"Author Name"},{"title":"源码编译OpenJdk11并使用Clion调试","slug":"compile-jdk11-and-debug-use-clion","date":"2019-09-01T02:50:43.000Z","updated":"2019-09-11T14:07:16.000Z","comments":true,"path":"2019/09/01/compile-jdk11-and-debug-use-clion/","link":"","permalink":"http://yoursite.com/2019/09/01/compile-jdk11-and-debug-use-clion/","excerpt":"","text":"下载jdk源码使用 Mercurial 下载jdk源码Mercurial是一个分布式版本控制系统, 可以把它理解成git 12345$ sudo apt install mercurial$ hg clone https://hg.openjdk.java.net/jdk/jdk11/ $ cd jdk11$ chmod u+x get_source.sh $ ./get_source.sh ## 开始下载源码 如果网络不稳定建议多试几次,get_source.sh是支持断点续传的 直到不出现如下提示即下载完整了 12WARNING: langtools exited abnormally (255)WARNING: nashorn exited abnormally (255) 使用 docker 下载jdk源码 $ docker run –rm -it -v {download_dir}:/output bolingcavalry/openjdksrc11:0.0.1 其中{download_dir}表示下载的位置,需提前创建, 然后cd {download_dir}, tar -zxvf 解压即可得到源码 编译准备 $ sudo apt-get install autoconf $ sudo apt-get install libx11-dev libxext-dev libxrender-dev libxtst-dev libxt-dev $ sudo apt-get install libcups2-dev $ sudo apt-get install libfontconfig1-dev $ sudo apt-get install libasound2-dev 由于jdk的编译需要前一个版本jdk参与因此需要先安装openjdk-10下载openjdk10 解压jdk-10_linux-x64_bin_ri.tar.gz $ tar -zxvf jdk-10_linux-x64_bin_ri.tar.gz然后在 ~/.bashrc末尾添加 (这里我经常看到有教程喜欢把配置写在/etc/profile或者/etc/environment 个人不喜欢把这种配置写在全局生效的地方,而且写在这种地方很容易重启后出问题) 123export JAVA_HOME= &#123;openjdk10目录&#125;export JRE_HOME=$JAVA_HOME/libexport PATH=$JAVA_HOME/bin:$PATH $ source ~/.bashrc 123openjdk version &quot;10&quot; 2018-03-20OpenJDK Runtime Environment 18.3 (build 10+44)OpenJDK 64-Bit Server VM 18.3 (build 10+44, mixed mode) 然后 $ cd {jdk11源码目录}$ chmod u+x config$ ./configure –disable-warnings-as-errors 1234567891011121314151617Configuration summary:* Debug level: release* HS debug level: product* JVM variants: server* JVM features: server: &apos;aot cds cmsgc compiler1 compiler2 epsilongc g1gc graal jfr jni-check jvmci jvmti management nmt parallelgc serialgc services vm-structs&apos; * OpenJDK target: OS: linux, CPU architecture: x86, address length: 64* Version string: 11-internal+0-adhoc.parallels.jdk11 (11-internal)Tools summary:* Boot JDK: openjdk version &quot;10.0.2&quot; 2018-07-17 OpenJDK Runtime Environment 18.3 (build 10.0.2+13) OpenJDK 64-Bit Server VM 18.3 (build 10.0.2+13, mixed mode) (at /opt/jdk-10.0.2)* Toolchain: gcc (GNU Compiler Collection)* C Compiler: Version 7.3.0 (at /usr/bin/gcc)* C++ Compiler: Version 7.3.0 (at /usr/bin/g++)Build performance summary:* Cores to use: 4* Memory limit: 5955 MB 编译JDK11至少4G内存. 因为JDK10没有带jre,因此编译时可能会报 ERROR: {JDK_HOME}/jre/lib No such file or directory 123456$ cd &#123;JDK_HOME&#125;$ mkdir jre$ `cp -r lib jre/```` 然后就可以在jdk源码目录 `$ make` Creating support/modules_cmds/jdk.pack/pack200 from 1 file(s)Creating support/modules_cmds/jdk.pack/unpack200 from 7 file(s)Creating support/modules_cmds/jdk.rmic/rmic from 1 file(s)Creating support/modules_cmds/jdk.scripting.nashorn.shell/jjs from 1 file(s)Creating support/modules_libs/jdk.sctp/libsctp.so from 3 file(s)Creating support/modules_libs/jdk.security.auth/libjaas.so from 1 file(s)Compiling 4 files for BUILD_JIGSAW_TOOLSStopping sjavac serverFinished building target ‘default (exploded-image)’ in configuration ‘linux-x86_64-normal-server-release’ 123![compiling-openjdk](https://image-1252310512.cos.ap-beijing.myqcloud.com/img/compiling-htop.png)编译好的东西就在 `jdk11/build/linux-x86_64-normal-server-release` $ lsbin conf include lib modules _packages_attribute.done release 12345678910111213141516然后 `cd bin &amp;&amp; ./java --version` , 出现的信息是带你用户名的哦~ 例如我用户名是`parallels`![java-version](https://image-1252310512.cos.ap-beijing.myqcloud.com/img/java-version.jpg)### 使用Clion调试JDK源码打开`Clion`,如果未安装Clion可以到官网[下载Clion](https://www.jetbrains.com/clion/)选择`New Cmake Project from Sources`,目录选择jdk源码路径的`src`文件夹,然后无脑全选![import-src](https://image-1252310512.cos.ap-beijing.myqcloud.com/img/import-jdk11-src.png &quot;import-src&quot;)然后Clion会自动Load Cmake Project和索引文件Load的时候会出现一个错误 gcc: error trying to exec ‘cc1obj’: execvp: No such file or directory 123~~原因似乎是缺少了 Objective C编译器~~,具体不太清楚,google了一下测试了发现安装`gobjc++`就好了&gt; $ sudo apt-get install gobjc++ – Configuring done– Generating done– Build files have been written to: /home/parallels/athos/jdk11/jdk11/src/cmake-build-debug 123456789101112131415161718192021222324252627282930313233然后选择 `Edit Configuations`![run/debug-configurations](https://image-1252310512.cos.ap-beijing.myqcloud.com/img/edit-configurations.png)`Executable` 选择 *build/linux-x86_64-normal-server-release/jdk/bin/* 下的`java`![executable-java](https://image-1252310512.cos.ap-beijing.myqcloud.com/img/executable.png)`Project Arguments` 填 `--version`然后再把`Build` Remove掉![remove-build](https://image-1252310512.cos.ap-beijing.myqcloud.com/img/build.png)完整配置如下:![debug-config](https://image-1252310512.cos.ap-beijing.myqcloud.com/img/debug-config.png)这里我在`jni.cpp` 中的`JNI_CreateJavaVM_inner`方法中的语句`result = Threads::create_vm((JavaVMInitArgs*) args, &amp;can_try_again);` 打上断点然后点击`Debug` 即可开始调试期间Clion会提示 `Breakpoint will not currently be hit. No executable code is associated`,意思是当前断点的代码不会命中, 事实上 **这是一个假情报,无视即可**然后就可以看到神器的画面了 ![debug](https://image-1252310512.cos.ap-beijing.myqcloud.com/img/debug.png)### 与java程序联合调试如果要 **修改** jdk源码进行调试 Java 11 版本中最令人兴奋的功能之一是增强 Java 启动器，使之能够运行单一文件的 Java 源代码。此功能允许使用 Java 解释器直接执行 Java 源代码。源代码在内存中编译，然后由解释器执行。唯一的约束在于所有相关的类必须定义在同一个 Java 文件中。 此功能对于开始学习 Java 并希望尝试简单程序的人特别有用，并且能与 jshell 一起使用，将成为任何初学者学习语言的一个很好的工具集。不仅初学者会受益，专业人员还可以利用这些工具来探索新的语言更改或尝试未知的 API。 如今单文件程序在编写小实用程序时很常见，特别是脚本语言领域。从中开发者可以省去用 Java 编译程序等不必要工作，以及减少新手的入门障碍。在基于 Java 10 的程序实现中可以通过三种方式启动： 作为 * .class 文件作为 * .jar 文件中的主类作为模块中的主类而在最新的 Java 11 中新增了一个启动方式，即可以在源代码中声明类，例如：如果名为 HelloWorld.java 的文件包含一个名为 hello.World 的类，那么该命令： $ java HelloWorld.java 也等同于： $ javac HelloWorld.java$ java -cp . hello.World 1234567891011121314151617181920- 选择 `Edit Configuations` - 点击`+`号 -&gt; `Cmake Application` - `Executable` 选择 *build/linux-x86_64-normal-server-release/jdk/bin/* 下的`java` - `Project Arguments` 填 `.java` 文件 ![cmake](https://image-1252310512.cos.ap-beijing.myqcloud.com/img/cmake-application.png)- **增加一个 External tool** **build jvm**![build-jvm](https://image-1252310512.cos.ap-beijing.myqcloud.com/img/external-tool.jpg)然后修改源码, 例如我在修改 `jvm.cpp : 475 行````cppJVM_ENTRY_NO_ENV(jlong, JVM_TotalMemory(void)) JVMWrapper(&quot;JVM_TotalMemory&quot;); size_t n = Universe::heap()-&gt;capacity(); //return convert_size_t_to_jlong(n); return 10086; hello.java 12345public class hello&#123; public static void main(String[] args)&#123; System.out.println(Runtime.getRuntime().totalMemory()); &#125;&#125; debug 参考: 编译与调试 OpenJDKmac下编译openjdk1.9及集成clion动态调试Ubuntu下编译OpenJDK9利用Ubuntu18编译openjdk11","categories":[],"tags":[],"author":"Huangxuny1"},{"title":"Netty 粘包解决","slug":"netty-sticky-pack","date":"2019-08-18T03:10:00.000Z","updated":"2019-10-04T14:28:16.000Z","comments":true,"path":"2019/08/18/netty-sticky-pack/","link":"","permalink":"http://yoursite.com/2019/08/18/netty-sticky-pack/","excerpt":"","text":"什么是粘包?TCP是基于字节流的，虽然应用层和TCP传输层之间的数据交互是大小不等的数据块，但是TCP把这些数据块仅仅看成一连串无结构的字节流，没有边界；另外从TCP的帧结构也可以看出，在TCP的首部没有表示数据长度的字段，基于上面两点，在使用TCP传输数据时，才有粘包或者拆包现象发生的可能s 通过一个例子来了解粘包ServerHandler.java 1234567public class ServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; ByteBuf byteBuf = (ByteBuf) msg; System.out.println( \"Server Read -&gt;\" + byteBuf.toString(StandardCharsets.UTF_8)); &#125;&#125; ClientHandler.java 12345678910111213141516public class ClientHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelActive(ChannelHandlerContext ctx) &#123; for (int i = 0; i &lt; 1000; i++) &#123; ByteBuf buffer = getByteBuf(ctx); ctx.channel().writeAndFlush(buffer); &#125; &#125; private ByteBuf getByteBuf(ChannelHandlerContext ctx) &#123; byte[] bytes = \"粘包Rua~\".getBytes(StandardCharsets.UTF_8); ByteBuf buffer = ctx.alloc().buffer(); buffer.writeBytes(bytes); return buffer; &#125;&#125; 理论上这段代码执行结果应该是 : 123Server Read -&gt;粘包Rua~ ... x98Server Read -&gt;粘包Rua~ 但实际上执行结果 ![sticky](https://image-1252310512.cos.ap-beijing.myqcloud.com/img/粘包 console.png) 这里粘包分为几种情况 1. 2.","categories":[],"tags":[{"name":"netty","slug":"netty","permalink":"http://yoursite.com/tags/netty/"},{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"},{"name":"sticky-pack","slug":"sticky-pack","permalink":"http://yoursite.com/tags/sticky-pack/"},{"name":"粘包","slug":"粘包","permalink":"http://yoursite.com/tags/%E7%B2%98%E5%8C%85/"}],"author":"Huangxuny1"},{"title":"红黑树学习","slug":"red-black-tree","date":"2019-08-04T04:16:23.000Z","updated":"2019-09-04T13:21:45.000Z","comments":true,"path":"2019/08/04/red-black-tree/","link":"","permalink":"http://yoursite.com/2019/08/04/red-black-tree/","excerpt":"Cut out summary from your post content here.","text":"Cut out summary from your post content here. The remaining content of your post.","categories":[],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"红黑树","slug":"红黑树","permalink":"http://yoursite.com/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/"}],"author":"Author Name"},{"title":"Kafka分布式环境搭建以及学习kafka","slug":"kafka","date":"2019-08-04T04:15:15.000Z","updated":"2019-09-04T13:21:14.000Z","comments":true,"path":"2019/08/04/kafka/","link":"","permalink":"http://yoursite.com/2019/08/04/kafka/","excerpt":"","text":"未完成 kafka Kafka是一个由Scala和Java编写的易于拓展的分布式流计算平台,目的是为处理实时数据提供一个统一、高吞吐、低延迟的平台。其持久化层本质上是一个”按照分布式事务日志架构的大规模发布/订阅消息队列” 相关术语 Topic 用来对消息进行分类，每个进入到Kafka的信息都会被放到一个Topic下 Broker 用来实现数据存储的主机服务器,即kafka节点,一个或多个Broker可以组成kafka集群 Partition 每个Topic中的消息会被分为若干个Partition，以提高消息的处理效率,每个Partition的内部是有序的 Producer 消息的生产者,向Broker发送消息 Consumer 消息的消费者,向Broker拉取消息 Consumer Group 消息的消费群组 replicas：partition 的副本，保障 partition 的高可用 leader：replicas 中的一个角色， producer 和 consumer 只跟 leader 交互 follower：replicas 中的一个角色，从 leader 中复制数据，作为副本，一旦 leader 挂掉，会从它的 followers 中选举出一个新的 leader 继续提供服务 controller：Kafka 集群中的其中一个服务器，用来进行 leader election 以及 各种 failover ZooKeeper：Kafka 通过 ZooKeeper 来存储集群的 meta 信息等 ZooKeeper ZooKeeper 是一个开源的分布式协调服务,其设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。 ZooKeeper 是一个典型的分布式数据一致性解决方案，分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。 kafka分布式环境搭建 node1 -&gt; ubuntu server 18.04.3LTS ip : 172.16.80.138 node2 -&gt; ubuntu server 18.04.3LTS ip : 172.16.80.139 node3 -&gt; ubuntu server 18.04.3LTS ip : 172.16.80.140 下载Zookeeper下载kafka [参考资料] 深入浅出理解基于 Kafka 和 ZooKeeper 的分布式消息队列 阿里大牛实战归纳——Kafka架构原理 Kafka 常用命令总结 Kafka 单机和分布式环境搭建与案例使用","categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"},{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}],"author":"Author Name"},{"title":"ubuntu 18.04 搭建K8s集群","slug":"k8s","date":"2019-07-18T06:52:05.000Z","updated":"2019-09-26T12:57:19.000Z","comments":true,"path":"2019/07/18/k8s/","link":"","permalink":"http://yoursite.com/2019/07/18/k8s/","excerpt":"","text":"未完成 Kubernetes（常简称为K8s）是用于自动部署、扩展和管理容器化（containerized）应用程序的开源系统。它旨在提供“跨主机集群的自动部署、扩展以及运行应用程序容器的平台”。它支持一系列容器工具, 包括Docker等。 PodKubernetes的基本调度单元称为“pod”。通过该种抽象类别可以把更高级别的抽象内容增加到容器化组件。一个pod一般包含一个或多个容器，这样可以保证它们一直位于主机上，并且可以共享资源。 NodeNode是Kubernetes中的工作节点，最开始被称为minion。一个Node可以是VM或物理机。每个Node（节点）具有运行pod的一些必要服务，并由Master组件进行管理，Node节点上的服务包括Docker、kubelet和kube-proxy。 Service一个 Service 在 Kubernetes 中是一个 REST 对象，和 Pod 类似。 像所有的 REST 对象一样， Service 定义可以基于 POST 方式，请求 apiserver 创建新的实例。 例如，假定有一组 Pod，它们对外暴露了 9376 端口，同时还被打上 “app=MyApp” 标签。 1234567891011kind: ServiceapiVersion: v1metadata: name: my-servicespec: selector: app: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 上述配置将创建一个名称为 “my-service” 的 Service 对象，它会将请求代理到使用 TCP 端口 9376，并且具有标签 “app=MyApp” 的 Pod 上。 这个 Service 将被指派一个 IP 地址（通常称为 “Cluster IP”），它会被服务的代理使用（见下面）。 该 Service 的 selector 将会持续评估，处理结果将被 POST 到一个名称为 “my-service” 的 Endpoints 对象上。 kubectlkubectl (kubenetes control) 是用于针对Kubernetes集群运行命令的命令行接口。本概述涵盖kubectl语法，描述命令操作，并提供常见的示例。有关每个命令的详细信息，包括所有支持的flags和子命令 kubeletKubelet负责每个节点的运行状态（即确保节点上的所有容器都正常运行）。它按照控制面板的指示来处理启动，停止和维护应用程序容器。 Kubelet会监视pod的状态，如果不处于所需状态，则pod将被重新部署到同一个节点。节点状态每隔几秒就会传递消息至中继主机。主控器检测到节点故障后，复制控制器将观察此状态更改，并在其他健康节点上启动pod。 kubeadmKubeadm 是一个工具，通过提供 kubeadm init 和 kubeadm join 来作为创建 Kubernetes 集群的最佳实践“快速路径”。 kubeadm 执行必要的操作来启动并运行一个最小可行集群。按照设计，它只关心引导，而不关心配置机器。同样，安装各种不同的插件，如 Kubernetes Dashboard、监控解决方案以及云特定的插件，都不在范围之内。 相反，我们期望用户能够基于 kubeadm 构建更高级别和更加定制化的工具，理想情况下，使用 kubeadm 作为所有部署的基础，可以更容易地创建一致的集群。 minikubeMinikube 是一个可以在本地轻松运行 Kubernetes 的工具。Minikube 会在您的笔记本电脑中的虚拟机上运行一个单节点的 Kubernetes 集群，以便用户对 Kubernetes 进行试用或者在之上进行 Kubernetes 的日常开发。 Kubernetes 集群部署安装docker $ wget -qO- https:get.docker.io/ | sh 安装kubelet kubeadm kubectl12345678910$ apt update &amp;&amp; apt install -y apt-transport-https$ sudo curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -$ sudo tee /etc/apt/sources.list.d/kubernetes.list &lt;&lt;-&apos;EOF&apos;deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial mainEOF$ apt update$ apt install -y kubelet kubeadm kubectl 刚安装完的时候使用systemctl status kubelet 查看kubelet状态时kubelet是没有正常启动的 12345678910# ---输出信息---● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: activating (auto-restart) (Result: exit-code) since Thu 2019-09-26 09:45:36 CST; 6s ago Docs: https://kubernetes.io/docs/home/ Process: 23979 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET Main PID: 23979 (code=exited, status=255) 原因是没有配置文件,不过不用担心待会使用kubeadm创建集群时会自动生成配置可以使用 sudo journalctl -f -u kubelet查看log分析原因 (注意sudo) 下载docker镜像由于国内无法访问到 google 相关的镜像仓库,因此需要先下载对应的docker镜像 $ kubeadm config images list 得到创建集群所需要的images 12345678910# ---输出信息---W0926 01:12:48.539605 110892 version.go:101] could not fetch a Kubernetes version from the internet: unable to get URL &quot;https://dl.k8s.io/release/stable-1.txt&quot;: Get https://dl.k8s.io/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)W0926 01:12:48.539672 110892 version.go:102] falling back to the local client version: v1.16.0k8s.gcr.io/kube-apiserver:v1.16.0k8s.gcr.io/kube-controller-manager:v1.16.0k8s.gcr.io/kube-scheduler:v1.16.0k8s.gcr.io/kube-proxy:v1.16.0k8s.gcr.io/pause:3.1k8s.gcr.io/etcd:3.3.15-0k8s.gcr.io/coredns:1.6.2 然后编写一个shell脚本, 从阿里云镜像仓库下载对应镜像后重新打tag实现 get-k8s-images.sh 1234567891011121314151617181920212223#!/bin/bashAPISERVER=v1.16.0MANAGER=v1.16.0SCHEDULER=v1.16.0PROXY=v1.16.0PAUSE=3.1ETCD=3.3.15-0COREDNS=1.6.2docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:$APISERVER docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:$MANAGERdocker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:$SCHEDULER docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:$PROXYdocker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:$PAUSEdocker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:$ETCDdocker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:$COREDNSdocker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:$APISERVER k8s.gcr.io/kube-apiserver:$APISERVERdocker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:$MANAGER k8s.gcr.io/kube-controller-manager:$MANAGERdocker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:$SCHEDULER k8s.gcr.io/kube-scheduler:$SCHEDULERdocker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:$PROXY k8s.gcr.io/kube-proxy:$PROXYdocker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:$PAUSE k8s.gcr.io/pause:$PAUSEdocker tag registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:$ETCD k8s.gcr.io/etcd:$ETCDdocker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:$COREDNS k8s.gcr.io/coredns:$COREDNS 然后给脚本赋予执行权限 $ chmod u+x get-k8s-images.sh$ ./get-k8s-images 然后耐心等待 … 创建k8s集群先关闭 swap 临时关闭 sudo swapoff -a永久关闭 sudo vim /etc/fstab 把 /swapfile 对应的行 使用# 注释掉 重启后生效 然后就可以使用kubeadm创建master节点了 $ sudo kubeadm init –pod-network-cidr 10.244.0.0/16 # –kubernetes-version 1.16.0 12345678910111213141516171819202122232425262728# ---输出信息----[preflight] Pulling images required for setting up a Kernetes cluster[preflight] This might take a minute or two, dependingn the speed of your internet connection.............. 这里需要稍等一会the control plane as static Pods from directory &quot;/etc/bernetes/manifests&quot;. This can take up to 4m0s[kubelet-check] Initial timeout of 40s passed.[apiclient] All control plane components are healthy aer 80.100064 seconds[upload-config] Storing the configuration used in ConfMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace ............................Your Kubernetes control-plane has initialized successfly!To start using your cluster, you need to run the follong as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/coig sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of t options listed at: https://kubernetes.io/docs/concepts/cluster-administtion/addons/Then you can join any number of worker nodes by runninthe following on each as root:kubeadm join 172.16.80.140:6443 --token d4bj0d.8v2f2a8br82tzq \\ --discovery-token-ca-cert-hash sha256:543feab975c7d43821abadfd02a8a718b84b11ff108a18e894a66c372358f8 安装结束后会有提示设置普通用户权限,否则无法在普通用户下使用 kubectl相关的命令 123$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config 此时 使用kubectl get pods --all-namespaces 是除了 coredns 之外的pods全都处于 1/1 ready 状态的大概是这样 : 123456789101112$ kubectl get pods --all-namespaces -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkube-system coredns-5644d7b6d9-fl7h8 0/1 Pending 0 25m &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;kube-system coredns-5644d7b6d9-rvh6q 0/1 Pending 0 25m &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;kube-system etcd-pr02 1/1 Running 0 24m 178.xxx.xxx.xxx pr02 &lt;none&gt; &lt;none&gt;kube-system kube-apiserver-pr02 1/1 Running 0 24m 178.xxx.xxx.xxx pr02 &lt;none&gt; &lt;none&gt;kube-system kube-controller-manager-pr02 1/1 Running 0 24m 178.xxx.xxx.xxx pr02 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-wwsm7 1/1 Running 0 20m 178.xxx.xxx.xxx pr02 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-4c9n4 1/1 Running 0 25m 178.xxx.xxx.xxx pr02 &lt;none&gt; &lt;none&gt;kube-system kube-scheduler-pr02 1/1 Running 0 24m 178.xxx.xxx.xxx pr02 &lt;none&gt; &lt;none&gt; 原因是我们还未设置网络方案 这里我们选择 flannel 方案 应用flannel网路 $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 如果get pods时 flannel 相关的pods显示Error: ErrImagePull或者Init:ImagePullBackOff 说明由于网络原因 flannel docker image 下载未成功. 所以需要手动自行pull镜像然后修改tag 首先获取到所需要的 flannel image $ kubectl get pods –all-namespaces拿到 下载镜像失败的pod的Name,类似于 kube-flannel-ds-amd64-5njgh,然后$ kubectl describe pods kube-flannel-ds-amd64-5njgh –namespace kube-system在 Event 中可以看到类似信息 123456Events:Type Reason Age From Message---- ------ ---- ---- -------Normal BackOff 108s kubelet, k8s-node2 Back-off pulling image &quot;quay.io/coreos/flannel:v0.11.0-amd64&quot; 然后既可以去 阿里云镜像仓库 或者 docker hub 下载对应的镜像 例如这里我在阿里云镜像仓库 找到了对应版本的镜像 $ docker pull registry.cn-hangzhou.aliyuncs.com/wy18301/flannel-v0.11.0-amd64重新打 tag$ docker tag registry.cn-hangzhou.aliyuncs.com/wy18301/flannel-v0.11.0-amd64:v0.11.0-amd64 quay.io/coreos/flannel:v0.11.0-amd64 然后还有一个巨大的坑. 困扰了我好久 最后在github issue找到的解决方法 在 /etc/cni/net.d/10-flannel.conflist中加入 &quot;cniVersion&quot;: &quot;0.2.0&quot;, 稍等一会,再使用 kubectl get pods --all-namespaces 查看pod状态就是全部ready了使用 kubectl get nodes查看master状态显示已就绪 Node加入集群master节点创建完成之后会有一段提示 12345Then you can join any number of worker nodes by runninthe following on each as root:kubeadm join 172.16.80.140:6443 --token d4bj0d.8v2f2a8br82tzq \\ --discovery-token-ca-cert-hash sha256:543feab975c7d43821abadfd02a8a718b84b11ff108a18e894a66c372358f8 因此在安装好环境和下载好镜像的Node中执行master节点给出的命令即可 如果token过期 可以使用kubeadm token create --print-join-command获得最新token 1234$ kubeadm token create --print-join-commandkubeadm join 172.16.80.140:6443 --token 1ijd4x.r65ft5c42kvmd3tv --discovery-token-ca-cert-hash sha256:543feab975c722d43821abadfd02a8a718b84b11ff108a18e894a66c372358f8 值得注意的是 : 节点中也要在 /etc/cni/net.d/10-flannel.conflist中加入 “cniVersion”: “0.2.0”, 然后就可以使用 kubectl get nodes 查看集群状态了 todo 常用命令todo 部署todo 升级镜像","categories":[],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/tags/k8s/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"},{"name":"阿里云镜像","slug":"阿里云镜像","permalink":"http://yoursite.com/tags/%E9%98%BF%E9%87%8C%E4%BA%91%E9%95%9C%E5%83%8F/"}],"author":"Huangxuny1"},{"title":"Docker Jenkins自动构建","slug":"docker-jenkins","date":"2019-07-13T20:38:24.000Z","updated":"2019-07-28T14:11:05.000Z","comments":true,"path":"2019/07/14/docker-jenkins/","link":"","permalink":"http://yoursite.com/2019/07/14/docker-jenkins/","excerpt":"","text":"Docker &nbsp;&nbsp;&nbsp;&nbsp;Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的镜像中，然后发布到任何流行的 Linux或Windows 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。 &nbsp;&nbsp;&nbsp;&nbsp; 之所以使用Docker运行Jenkins,是因为即可以通过Docker快速部署,又可以轻松把一个Docker容器里的资料在另一个设备中轻松备份Jenkins环境与配置,还可以隔离物理机环境,使Jenkins构架环境不受本地环境影响,当然还有很重要的一点,可以通过Docker实现快速的配置多Jenkins节点. 安装Docker个人喜欢用官方脚本安装Docker 1$ wget -qO- https://get.docker.com/ | sh 安装中会提示输入 root密码 ,安装结束后还会提示把 当前用户 添加到docker用户组中 1$ sudo usermod -aG docker $USERNAME # 该命令reboot后生效 这样就可以在 当前用户 中使用 $ docker COMMAND 而不会出现 Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.39/containers/json: dial unix /var/run/docker.sock: connect: permission denied Github Webhooks&nbsp;&nbsp;&nbsp;&nbsp; Github Webhooks可以理解为代码仓库的钩子,当订阅了Github中的某些事件过,这些事件被触发(如:git push),则将会向配置的URL发送一个HTTP POST请求.通过这个请求,可以利用Jenkins自动建构,部署. You&#39;re only limited by your imagination. 具体Webhooks介绍以及其所支持的构建方式可以参考 : Github Webhooks Jenkins &nbsp;&nbsp;&nbsp;&nbsp;Jenkins是开源CI&amp;CD软件领导者， 提供超过1000个插件来支持构建、部署、自动化， 满足任何项目的需要。Jenkins 支持各种运行方式，可通过系统包、Docker 或者通过一个独立的 Java 程序。 拉取 Jenkins 官方镜像1$ docker pull jenkins/jenkins &nbsp;&nbsp;&nbsp;&nbsp; 不推荐使用 docker pull jenkins 因为在docker-hub 中 : This image has been deprecated in favor of the jenkins/jenkins:lts image provided and maintained by Jenkins Community as part of project’s release process. The images found here will receive no further updates after LTS 2.60.x. Please adjust your usage accordingly. 即 : 该镜像已经被弃用,取而代之的是 jenkins/jenkins:lts 但文档还是推荐阅读 jenkins 如果镜像拉取较慢,可以使用 Aliyun镜像加速服务 或者使用","categories":[],"tags":[{"name":"jenkins","slug":"jenkins","permalink":"http://yoursite.com/tags/jenkins/"},{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"},{"name":"Github Webhooks","slug":"Github-Webhooks","permalink":"http://yoursite.com/tags/Github-Webhooks/"}],"author":"Huangxuny1"},{"title":"使用hugo+Github Pages搭建个人静态博客","slug":"hugo-github-pages","date":"2019-07-13T01:29:26.000Z","updated":"2019-09-04T13:09:46.000Z","comments":true,"path":"2019/07/13/hugo-github-pages/","link":"","permalink":"http://yoursite.com/2019/07/13/hugo-github-pages/","excerpt":"","text":"Hugo hugo是由Go语言编写的开源静态站点生成器,是世界上最快的静态网站引擎之一,旨在为网站用户提供最佳的浏览体验，并为网站作者提供理想的写作体验. Install Hugo 由于Hugo是Golang编写,可以在任何能运行Golang编译器工具链的机器上安装Hugo. 因此在MacOS、Windows、Linux、OpenBSD、FreeBSD都能够安装Hugo. Homebrew (macOS)1brew install hugo Debian and Ubuntu1sudo apt-get install hugo Archlinux1sudo pacman -Syu hugo Fedora, Red Hat and CentOS1sudo dnf install hugo Windows从Hugo Release中下载对应的编译好的Windows版本的Hugo可执行文件,然后将其/bin目录添加到 环境变量 即可. 安装完成后, 执行命令 hugo version 如果安装成功会输入类似信息: Hugo Static Site Generator v0.55.4/extended darwin/amd64 BuildDate: unknown 创建个人博客1hugo new site &#123;BlogName&#125; 其中 {YourBlogName} 可以是 &lt;用户名&gt;.github.io 也可以 随意 然后 : 1cd &#123;BlogName&#125; 会看见如下目录结构: 123456789.├── archetypes │ └── default.md # 使用hugo new命令在hugo中创建新的内容模板├── config.toml # 配置文件,里面有丰富的配置信息├── content # 博客文章的 markdown文件├── data # 此目录用于存储生成网站时雨果可以使用的配置文件。├── layouts # 以.html文件的形式存储模板，指定如何将内容视图呈现到静态网站中。├── static # 静态资源包括 images, CSS, JavaScript 等└── themes # 主题文件 设置主题选主题 可以在hugo-theme 中选择心仪的主题,个人比较喜欢AllinOne,以 AllinOne 为例 : 使用 git clone 到themes/下,这样可以自定义主题. 1git clone https://github.com/orianna-zzo/AllinOne.git themes/AllinOne 使用 git submodule 这样对自动部署比较方面 ( 前提是在cd 的时候 git init) 1git submodule add https://github.com/orianna-zzo/AllinOne.git themes/AllinOne 配置主题cd themes/AllinOne/ 中会发现有一个 exampleSite 这里就是一个Demo,可以从demo的 config.toml 里面copy配置文件到我们的配置文件中. 更多关于config.toml的配置,可以查阅官方文档 hugo-configuration. 本地测试Hugo自带服务器，可以用命令行启动： 1hugo server -D 服务启动后访问 http://localhost:1313访问,自带的服务器可以热加载文件的改动,监控文章的改动从而自动去加载. 新建文章根据config.toml中的 12345[[menu.main]] name = \"Blog\" weight = -120 identifier = \"blog\" url = \"blog/\" 可知Blog是存放在 blog/ 下,因此执行: 1hugo new blog/HelloWorld.md 随后会输出 {BlogBaseDir}/content/blog/helloWord.md created 同理我们可以从 exampleSite 的/content/blog copy一些文章内容信息,例如: 123456789101112---date: \"2018-08-18T20:14:59+08:00\"publishdate: \"2018-08-19+08:00\"lastmod: \"2018-08-18+08:00\"draft: falsetitle: \"Blog养成记(14) 让同页滚动更平滑\"tags: [\"前端\", \"js\", \"blog\"]series: [\"Blog养成记\"]categories: [\"杂技浅尝\"]img: \"images/blog/2018-08/test1.jpg\"toc: true--- 然后就可以愉快的写作了. 部署到 Github PagesGithub Pages Github Pages 是面向用户、组织和项目开放的公共静态页面搭建托管服务，站点可以被免费托管在Github 上，你可以选择使用Github Pages 默认提供的域名github.io 或者自定义域名来发布站点。 新建git repository 在Github中新建名为 {用户名}.github.io 的仓库 Github Pages 有三种发布方式 You can configure GitHub Pages to publish your site’s source files from master, gh-pages, or a /docs folderon your master branch for Project Pages and other Pages sites that meet certain criteria. master分支 (如果仓库名是 {用户名}.github.io 那么只能通过此方式发布) gh-pages分支 master分支的 /docs 目录 由于这里我选择使用master分支发布Github Pages 在站点的根目录, (即 config.toml 所在的目录) 执行 : 1hugo 123456789101112 | EN+------------------+-----+ Pages | 80 Paginator pages | 3 Non-page files | 0 Static files | 108 Processed images | 0 Aliases | 1 Sitemaps | 1 Cleaned | 0Total in 137 ms 生成静态资源 值得注意的是 如果 .md 文章中 draft: true 那么使用 hugo生成静态页面的时候不会生成这篇文章如果需要把 草稿 一并生成可以使用可以在 hugo -h 中看到 1-D, --buildDrafts include content marked as draft 因此使用 1hugo -D 然后 1234567891011cd public/ git initgit add .git commit -m \"first commit\"git remote add origin &#123;GitRepoURL&#125;git push -u origin master 成功之后稍等片刻,访问 https://{用户名}.github.io 即可呈现内容 /docs 发布方式使用此方式的前提是 仓库名称不是 {用户名}.github.io 修改 config.toml 中的 publishdir publishdir = “docs” 那么使用 hugo -D 生成的静态页面就会在 docs/ 中生成 在仓库的 Settings-&gt;GitHub Pages-&gt;Source 选择 master branch /docs folder 即可 页面发布成功后可以通过 https://{用户名}.github.io/{仓库名} 访问","categories":[],"tags":[{"name":"blog","slug":"blog","permalink":"http://yoursite.com/tags/blog/"},{"name":"GithubPages","slug":"GithubPages","permalink":"http://yoursite.com/tags/GithubPages/"}],"author":"Huangxuny1"}]}